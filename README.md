# Natural-Language-Processing

Implementations of the NLP techniques, functions. Will start adding transformer code with datasets. Below will be some terms, questions I learned/ would like to be documented on the way.

## Exploding gradient 

Exploding gradients are a problem where large error gradients accumulate and result in very large updates to neural network model weights during training.

This has the effect of your model being unstable and unable to learn from your training data. Read more here : https://machinelearningmastery.com/exploding-gradients-in-neural-networks/ 

## Vanishning Gradient

In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, during each iteration of training each of the neural networks weights receives an update proportional to the partial derivative of the error function with respect to the current weight.
Read more here : https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484 

## Define cell state and cell gates in LSTM?
Read this article/page for a deep understanding : https://d2l.ai/chapter_recurrent-modern/lstm.html 
